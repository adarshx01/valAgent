# ETL Validation Agent ğŸ¤–

> **AI-powered Data Pipeline Validation System**

An intelligent agent for validating ETL (Extract, Transform & Load) jobs using natural language business rules. This enterprise-grade system automates the manual QA process by generating and executing comprehensive test cases against source and target databases.

## ğŸ¯ Overview

The ETL Validation Agent transforms how data quality is validated:

| **Current Process (Manual)** | **Future Process (Automated)** |
|------------------------------|-------------------------------|
| QA manually writes test cases | QA provides business rules in plain English |
| Manual SQL query writing | Agent generates SQL automatically |
| Manual query execution | Parallel execution for large datasets |
| Screenshots as proof | Detailed reports with execution proofs |
| Time-consuming & error-prone | Fast, consistent & comprehensive |

## âœ¨ Features

- **ğŸ—£ï¸ Natural Language Rules**: Define validation rules in plain English
- **ğŸ” Automatic Schema Analysis**: Extracts and analyzes source/target database schemas
- **ğŸ¤– AI-Powered Query Generation**: GPT-4 generates appropriate SQL test cases
- **âš¡ Parallel Processing**: Handles lakhs of records with PostgreSQL parallel queries
- **ğŸ“Š Comprehensive Reports**: Pass/fail results with proof of execution
- **ğŸ’¡ AI Analysis**: Intelligent root cause analysis and recommendations
- **ğŸŒ REST API**: Full-featured API for integration
- **ğŸ’» CLI Interface**: Command-line tool for automation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ETL Validation Agent                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FastAPI    â”‚  â”‚    CLI      â”‚  â”‚   Validation Agent      â”‚ â”‚
â”‚  â”‚  REST API   â”‚  â”‚  Interface  â”‚  â”‚   (Orchestrator)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                â”‚                      â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Service Layer                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚Schema Serviceâ”‚  â”‚ LLM Service  â”‚  â”‚ Executor Service â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                      â”‚                      â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      Core Layer                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   Database   â”‚  â”‚    Config    â”‚  â”‚   Exceptions     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   Manager    â”‚  â”‚   Settings   â”‚  â”‚                  â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                                              â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Source DB (PostgreSQL)      Target DB (PostgreSQL)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- PostgreSQL databases (source and target)
- OpenAI API key

### Installation

```bash
# Clone the repository
git clone https://github.com/iqvia/etl-validation-agent.git
cd etl-validation-agent

# Install with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

### Configuration

Create a `.env` file:

```env
# Database connections
SOURCE_DB_URI="postgresql://user:password@host:5432/source_db"
TARGET_DB_URI="postgresql://user:password@host:5432/target_db"

# OpenAI configuration
LLM_PROVIDER="openai"
OPENAI_API_KEY="sk-your-api-key"
OPENAI_MODEL="gpt-4.1"

# Optional settings
LOG_LEVEL="INFO"
MAX_PARALLEL_WORKERS=8
BATCH_SIZE=10000
```

### Running the Agent

#### Option 1: API Server

```bash
# Start the FastAPI server
python main.py serve

# Or with uv
uv run python main.py serve
```

Access the API documentation at `http://localhost:8000/docs`

#### Option 2: CLI

```bash
# Run validation with business rules
etl-validator validate --text "
1. All customer records should exist in target
2. Email addresses should be lowercase
3. Order totals should match between source and target
"

# Or from a file
etl-validator validate --rules business_rules.txt --output report.md

# Execute ad-hoc query
etl-validator query "SELECT COUNT(*) FROM customers" --db target

# Generate SQL from description
etl-validator generate-sql "Count active customers created this month"
```

## ğŸ“ Business Rules Examples

```text
1. Data Completeness:
   - All records from source.orders should exist in target.orders
   - No NULL values allowed in customer_email column

2. Data Transformation:
   - Email addresses must be lowercase in target
   - Phone numbers should be formatted as +1-XXX-XXX-XXXX
   - Dates should be converted to UTC timezone

3. Data Consistency:
   - Total order amounts should match: source vs target
   - Customer counts should be equal across databases
   
4. Referential Integrity:
   - All order.customer_id should exist in customers.id
   
5. Aggregation Rules:
   - Sum of daily_sales should match monthly_sales total
```

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/validate` | Run full validation |
| POST | `/api/v1/validate/stream` | Streaming validation with progress |
| POST | `/api/v1/validate/quick` | Quick single-rule validation |
| POST | `/api/v1/query/execute` | Execute ad-hoc SQL |
| POST | `/api/v1/query/generate` | Generate SQL from description |
| GET | `/api/v1/schema/source` | Get source schema |
| GET | `/api/v1/schema/target` | Get target schema |
| GET | `/api/v1/schema/compare` | Compare schemas |
| GET | `/api/v1/databases/info` | Get database info |
| GET | `/health` | Health check |

### API Example

```python
import requests

# Run validation
response = requests.post(
    "http://localhost:8000/api/v1/validate",
    json={
        "business_rules": """
        1. All customer records should exist in target
        2. Email addresses should be lowercase
        """,
        "validation_name": "Customer Data Validation"
    }
)

result = response.json()
print(f"Status: {result['report']['overall_status']}")
print(f"Passed: {result['report']['summary']['passed']}")
print(f"Failed: {result['report']['summary']['failed']}")
```

## ğŸ“Š Sample Output

```markdown
# ETL Validation Report

**Report ID:** abc123
**Generated:** 2026-01-30T10:30:00Z

## Overview

- **Source Database:** source
- **Target Database:** target
- **Overall Status:** PASSED

## Execution Summary

| Metric | Value |
|--------|-------|
| Total Tests | 15 |
| Passed | 14 |
| Failed | 1 |
| Pass Rate | 93.3% |
| Total Duration | 2450ms |

## Scenarios Covered

- âœ… **Row Count Validation**: Verified record counts match
- âœ… **Data Transformation**: Checked email lowercase conversion
- âœ… **Null Check**: Validated no NULL values in required fields
- âŒ **Referential Integrity**: Found orphan records

## AI Analysis

The validation completed with 93.3% pass rate. One test failed 
due to 3 orphan order records referencing non-existent customers.
This appears to be a data sync timing issue.

## Recommendations

1. Investigate orphan records in orders table
2. Add foreign key constraint to prevent future issues
3. Consider adding a reconciliation job
```

## ğŸ› ï¸ Development

```bash
# Install dev dependencies
uv sync --all-extras

# Run tests
pytest

# Run with coverage
pytest --cov=src/etl_validator

# Type checking
mypy src/

# Linting
ruff check src/

# Formatting
black src/
```

## ğŸ“ Project Structure

```
etl-validation-agent/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl_validator/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cli.py              # CLI interface
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ config.py       # Configuration
â”‚       â”‚   â”œâ”€â”€ database.py     # Database manager
â”‚       â”‚   â””â”€â”€ exceptions.py   # Custom exceptions
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ schema.py       # Schema models
â”‚       â”‚   â”œâ”€â”€ rules.py        # Business rules
â”‚       â”‚   â”œâ”€â”€ test_case.py    # Test cases
â”‚       â”‚   â””â”€â”€ results.py      # Results & reports
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â”œâ”€â”€ llm_service.py  # AI/LLM integration
â”‚       â”‚   â”œâ”€â”€ schema_service.py
â”‚       â”‚   â”œâ”€â”€ executor_service.py
â”‚       â”‚   â””â”€â”€ validation_orchestrator.py
â”‚       â”œâ”€â”€ agents/
â”‚       â”‚   â””â”€â”€ validation_agent.py
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â”œâ”€â”€ app.py          # FastAPI app
â”‚       â”‚   â”œâ”€â”€ routes.py       # API routes
â”‚       â”‚   â””â”€â”€ dependencies.py
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ logger.py
â”‚           â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

## âš™ï¸ Configuration Options

| Variable | Description | Default |
|----------|-------------|---------|
| `SOURCE_DB_URI` | Source PostgreSQL connection | Required |
| `TARGET_DB_URI` | Target PostgreSQL connection | Required |
| `OPENAI_API_KEY` | OpenAI API key | Required |
| `OPENAI_MODEL` | Model to use | gpt-4.1 |
| `MAX_PARALLEL_WORKERS` | Parallel query workers | 8 |
| `BATCH_SIZE` | Batch size for large data | 10000 |
| `QUERY_TIMEOUT` | Query timeout (seconds) | 300 |
| `LOG_LEVEL` | Logging level | INFO |

## ğŸ”’ Security

- Database credentials stored as SecretStr (masked in logs)
- API rate limiting built-in
- Optional API key authentication
- CORS configuration for web clients

## ğŸ“ˆ Performance

- **Parallel Query Execution**: Configurable worker pool
- **Connection Pooling**: Efficient database connections
- **Batch Processing**: Handles large datasets in chunks
- **Async I/O**: Non-blocking operations throughout

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT-4 API
- FastAPI for the excellent web framework
- asyncpg for high-performance PostgreSQL access

---

**Built with â¤ï¸ by Adarsh**
